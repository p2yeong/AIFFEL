{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "typical-dress",
   "metadata": {},
   "source": [
    "# 영화리뷰 텍스트 감성분석하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-alarm",
   "metadata": {},
   "source": [
    "## 1. 텍스트 데이터의 특징"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-tender",
   "metadata": {},
   "source": [
    "### 1.1 텍스트를 숫자로 표현하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-times",
   "metadata": {},
   "source": [
    "단어와 그 단어의 의미를 나타내는 벡터를 짝지어 본다.\n",
    "\n",
    "아래의 문장을 예시로 텍스트 데이터를 처리해본다.\n",
    "- i feel hungry\n",
    "- i eat lunch\n",
    "- now i feel happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "humanitarian-miracle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'feel', 'hungry']\n"
     ]
    }
   ],
   "source": [
    "# 처리해야 할 문장을 파이썬 리스트에 옮겨 담는다.\n",
    "sentences = ['i feel hungry', 'i eat lunch', 'now i feel happy']\n",
    "\n",
    "# 단어 단위로 문장을 쪼개기\n",
    "word_list = 'i feel hungry'.split()\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-wholesale",
   "metadata": {},
   "source": [
    "사전을 만들기 위해 모든 문장을 단어 단위로 나누고, 파이썬 딕셔너리 자료구조로 표현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "integrated-parallel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<PAD>', 1: '<BOS>', 2: '<UNK>', 3: 'i', 4: 'feel', 5: 'hungry', 6: 'eat', 7: 'lunch', 8: 'now', 9: 'happy'}\n"
     ]
    }
   ],
   "source": [
    "index_to_word = {}\n",
    "\n",
    "# <BOS>, <PAD>, <UNK>는 관례적으로 딕셔너리 맨 앞에 넣어준다.\n",
    "index_to_word[0]='<PAD>'  # 패딩용 단어\n",
    "index_to_word[1]='<BOS>'  # 문장의 시작지점\n",
    "index_to_word[2]='<UNK>'  # 사전에 없는(Unknown) 단어\n",
    "index_to_word[3]='i'\n",
    "index_to_word[4]='feel'\n",
    "index_to_word[5]='hungry'\n",
    "index_to_word[6]='eat'\n",
    "index_to_word[7]='lunch'\n",
    "index_to_word[8]='now'\n",
    "index_to_word[9]='happy'\n",
    "\n",
    "print(index_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-reggae",
   "metadata": {},
   "source": [
    "데이터를 숫자로 바꾸기 위헤 딕셔너리를 {텍스트:인덱스} 구조로 변경한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "norwegian-portland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<BOS>': 1, '<UNK>': 2, 'i': 3, 'feel': 4, 'hungry': 5, 'eat': 6, 'lunch': 7, 'now': 8, 'happy': 9}\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "word_to_index={word:index for index, word in index_to_word.items()}\n",
    "print(word_to_index)\n",
    "print(word_to_index['feel']) # 단어 'feel'은 숫자 인덱스 4로 바뀐다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-glass",
   "metadata": {},
   "source": [
    "문장을 input으로 주면, 단어 인덱스 리스트로 변환해 주는 함수 만들기(모든 문장은 <BOS>로 시작하는 것으로 한다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "useful-volunteer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else  word_to_index['<UNK>'] for word in sentence.split()]\n",
    "print(get_encoded_sentence('i eat lunch', word_to_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-weekly",
   "metadata": {},
   "source": [
    "여러 개의 문장 리스트를 한번에 숫자 텐서로 encode 해주는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "embedded-national",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]]\n"
     ]
    }
   ],
   "source": [
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# sentences=['i feel hungry', 'i eat lunch', 'now i feel happy']\n",
    "encoded_sentences = get_encoded_sentences(sentences, word_to_index)\n",
    "print(encoded_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-survey",
   "metadata": {},
   "source": [
    "반대로, 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "appropriate-uzbekistan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i feel hungry\n"
     ]
    }
   ],
   "source": [
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "print(get_decoded_sentence([1, 3, 4, 5], index_to_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-entrance",
   "metadata": {},
   "source": [
    "여러 개의 숫자 벡터로 encode된 문장을 한번에 decode하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "incredible-conjunction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i feel hungry', 'i eat lunch', 'now i feel happy']\n"
     ]
    }
   ],
   "source": [
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]\n",
    "\n",
    "# encoded_sentences=[[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] 가 아래와 같이 변환됩니다.\n",
    "print(get_decoded_sentences(encoded_sentences, index_to_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-seating",
   "metadata": {},
   "source": [
    "### 1.2 Embedding 레이어의 등장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-offset",
   "metadata": {},
   "source": [
    "우리가 하려는 것은 단어와 그 단어의 의미를 나타내는 벡터를 짝짓는 것. 단어의 의미를 나타내는 벡터를 훈련 가능한 파라미터로 놓고 이를 딥러닝을 통해 학습해 최적화한다.\n",
    "Tensorflow, Pytorch 등의 딥러닝 프레임워크들은 이러한 의미벡터 파라미터를 구현한 Embedding 레이어를 제공한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-tampa",
   "metadata": {},
   "source": [
    "> embedding 레이어의 __인풋__ 이 되는 문장 벡터는 그 __길이가 일정__ 해야 한다.\n",
    "(Tensorflow에서는 keras.preprocessing.sequence.pad_sequences라는 편리한 함수를 통해 문장 벡터 뒤에 패딩(<PAD>)을 추가하여 길이를 일정하게 맞춰주는 기능을 제공)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "balanced-pipeline",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0.02907392  0.02859164 -0.0419425   0.00335587]\n",
      "  [-0.03963291  0.01236938 -0.04840344 -0.00640609]\n",
      "  [ 0.00242843  0.02733065 -0.00914029  0.04297358]\n",
      "  [ 0.0333668   0.00268817 -0.03770499  0.03853634]\n",
      "  [-0.0478671  -0.00416014 -0.03783959  0.00587008]]\n",
      "\n",
      " [[ 0.02907392  0.02859164 -0.0419425   0.00335587]\n",
      "  [-0.03963291  0.01236938 -0.04840344 -0.00640609]\n",
      "  [ 0.00338869  0.04658907 -0.04785929  0.01763048]\n",
      "  [-0.04648193 -0.04934604 -0.00517305  0.0009892 ]\n",
      "  [-0.0478671  -0.00416014 -0.03783959  0.00587008]]\n",
      "\n",
      " [[ 0.02907392  0.02859164 -0.0419425   0.00335587]\n",
      "  [ 0.01375936  0.0402511  -0.03943772 -0.0057791 ]\n",
      "  [-0.03963291  0.01236938 -0.04840344 -0.00640609]\n",
      "  [ 0.00242843  0.02733065 -0.00914029  0.04297358]\n",
      "  [ 0.02006575 -0.04041693 -0.01075566 -0.01085861]]], shape=(3, 5, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "vocab_size = len(word_to_index)  # 위 예시에서 딕셔너리에 포함된 단어 개수는 10\n",
    "word_vector_dim = 4    # 그림과 같이 4차원의 워드 벡터를 가정합니다.\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=word_vector_dim, mask_zero=True)\n",
    "\n",
    "# keras.preprocessing.sequence.pad_sequences를 통해 word vector를 모두 일정 길이로 맞춰주어야 \n",
    "# embedding 레이어의 input이 될 수 있음에 주의해 주세요. \n",
    "raw_inputs = np.array(get_encoded_sentences(sentences, word_to_index))\n",
    "raw_inputs = keras.preprocessing.sequence.pad_sequences(raw_inputs,\n",
    "                                                       value=word_to_index['<PAD>'],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=5)\n",
    "output = embedding(raw_inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-oliver",
   "metadata": {},
   "source": [
    "👀 shape=(3,5,4) 각 숫자의 의미는?\n",
    "- 3은 입력문장 개수\n",
    "- 5는 입력문장의 최대 길이\n",
    "- 4는 워드 벡터의 차원 수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-wesley",
   "metadata": {},
   "source": [
    "## 2. 시퀀스 데이터를 다루는 RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-integration",
   "metadata": {},
   "source": [
    "RNN은 시간의 흐름에 따라 새롭게 들어오는 입력에 따라 변하는 현재 상태를 묘사하는 state machine으로 설계\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-horse",
   "metadata": {},
   "source": [
    "!['stateful/stateless'](https://image.slidesharecdn.com/webhttprest-120910201409-phpapp02-130121190926-phpapp02/95/-49-1024.jpg?cb=1358795626)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-implementation",
   "metadata": {},
   "source": [
    "RNN 모델을 사용하여 이전 스템의 텍스트 데이터를 처리하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "manual-international",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 4)           40        \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 8)                 416       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 537\n",
      "Trainable params: 537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10 # 어휘 사전의 크기\n",
    "word_vector_dim = 4 # 단어 하나를 표현하는 임베딩 벡터의 차원수\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.LSTM(8))\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid')) # 최종 출력은 긍정/부정을 나타내는 1dim\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-estonia",
   "metadata": {},
   "source": [
    "## 3. CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-millennium",
   "metadata": {},
   "source": [
    "- 텍스트 처리를 하기위해 `1-D Convolution Neural Netword(1-D CNN)`를 사용할 수 있다.\n",
    "- `1-D CNN`은 문장 체페를 한꺼번에 한 방향으로 길이 n짜리 필터로 스캐닝 하면서 n단어 이내에서 발견되는 특징을 추출하고, 그것으로 문장을 분류하는 방식으로 사용된다.\n",
    "- CNN 계열은 RNN 계열보다 병렬처리가 효율적이기 때문에 학습 속도도 훨씬 빠르게 진행된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "disturbed-handbook",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 4)           40        \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 16)          464       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,457\n",
      "Trainable params: 2,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 4   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-valuation",
   "metadata": {},
   "source": [
    "아주 간단하게는 `GlobalMaxPooling1D()` 레이어 하나만 사용할 수 도 있다. 이 방식은 전체 문장 중에서 단 하나의 가장 중요한 단어만 피처로 추출하여 그것으로 문장의 긍/부정을 평가하는 방식이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "broad-istanbul",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 4)           40        \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 89\n",
      "Trainable params: 89\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 4   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-detector",
   "metadata": {},
   "source": [
    "## 4. IMDb 영화리뷰 감성분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-attribute",
   "metadata": {},
   "source": [
    "### 4.1 IMDB 데이터셋 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "constant-skirt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플 개수: 25000, 테스트 개수: 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)\n",
    "imdb = keras.datasets.imdb\n",
    "\n",
    "# IMDb 데이터셋 다운로드 \n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000) # 10,000개의 word_to_index 딕셔너리까지 생성된 형태로 데이터셋이 생성\n",
    "print(\"훈련 샘플 개수: {}, 테스트 개수: {}\".format(len(x_train), len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "arranged-drunk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "라벨:  1\n",
      "1번째 리뷰 문장 길이:  218\n",
      "2번째 리뷰 문장 길이:  189\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])  # 1번째 리뷰데이터\n",
    "print('라벨: ', y_train[0])  # 1번째 리뷰데이터의 라벨\n",
    "print('1번째 리뷰 문장 길이: ', len(x_train[0]))\n",
    "print('2번째 리뷰 문장 길이: ', len(x_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "pleased-female",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# imdb 데이터셋에는 encode에 사용한 딕셔너리를 제공한다\n",
    "word_to_index = imdb.get_word_index()\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "print(index_to_word[1])     # 'the' 가 출력됩니다. \n",
    "print(word_to_index['the'])  # 1 이 출력됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "upper-hearing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS>\n",
      "4\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "# IMDb 데이터셋의 텍스트 인코딩을 위한 word_to_index, index_to_word 보정\n",
    "#실제 인코딩 인덱스는 제공된 word_to_index에서 index 기준으로 3씩 뒤로 밀려 있습니다.  \n",
    "word_to_index = {k:(v+3) for k,v in word_to_index.items()}\n",
    "\n",
    "# 처음 몇 개 인덱스는 사전에 정의되어 있습니다\n",
    "word_to_index[\"<PAD>\"] = 0\n",
    "word_to_index[\"<BOS>\"] = 1\n",
    "word_to_index[\"<UNK>\"] = 2  # unknown\n",
    "word_to_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "index_to_word[0] = \"<PAD>\"\n",
    "index_to_word[1] = \"<BOS>\"\n",
    "index_to_word[2] = \"<UNK>\"\n",
    "index_to_word[3] = \"<UNUSED>\"\n",
    "\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "\n",
    "print(index_to_word[1])     # '<BOS>' 가 출력됩니다. \n",
    "print(word_to_index['the'])  # 4 이 출력됩니다. \n",
    "print(index_to_word[4])     # 'the' 가 출력됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "transsexual-jenny",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
      "라벨:  1\n"
     ]
    }
   ],
   "source": [
    "# decode 테스트\n",
    "print(get_decoded_sentence(x_train[0], index_to_word))\n",
    "print('라벨: ', y_train[0])  # 1번째 리뷰데이터의 라벨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-investor",
   "metadata": {},
   "source": [
    "`pad_sequences`를 통해 데이터셋 상의 문장의 길이를 통일하는 것을 잊어서는 안된다. 문장 최대 길이 maxlen의 값 설정도 전체 모델 성능에 영향을 미치게 된다. 적절한 값을 찾기 위해서는 전체 데이터 셋의 분포를 확인해 보는 것이 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "stretch-saturn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  234.75892\n",
      "문장길이 최대 :  2494\n",
      "문장길이 표준편차 :  172.91149458735703\n",
      "pad_sequences maxlen :  580\n",
      "전체 문장의 0.94536%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "# 텍스트데이터 문장을 담은 리스트 생성\n",
    "total_data_text = list(x_train) + list(x_test)\n",
    "\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "automated-return",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 580)\n"
     ]
    }
   ],
   "source": [
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        value=word_to_index[\"<PAD>\"],\n",
    "                                                        padding='pre', # 혹은 'pos'\n",
    "                                                        maxlen=maxlen)\n",
    "\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='pre',\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-employment",
   "metadata": {},
   "source": [
    "RNN은 입력데이터가 순차적으로 처리되어, 가장 마지막 입력이 최종 state값에 가장 영향을 많이 미치게 된다. 그러므로 마지막 입력이 무의미한 padding으로 채워지는 것은 비효율 적. 따라서 'pre' 가 훨씬 유리하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-paper",
   "metadata": {},
   "source": [
    "### 4.2 딥러닝 모델 설계와 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "friendly-looking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 160,145\n",
      "Trainable params: 160,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 16  # 워드 벡터의 차원 수 (변경 가능한 하이퍼파라미터)\n",
    "\n",
    "# model 설계 - 딥러닝 모델 코드를 직접 작성해 주세요.\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "spatial-stephen",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 580)\n",
      "(15000,)\n"
     ]
    }
   ],
   "source": [
    "# train, validation set 분리\n",
    "\n",
    "# validation set 10000건 분리\n",
    "x_val = x_train[:10000]   \n",
    "y_val = y_train[:10000]\n",
    "\n",
    "# validation set을 제외한 나머지 15000건\n",
    "partial_x_train = x_train[10000:]  \n",
    "partial_y_train = y_train[10000:]\n",
    "\n",
    "print(partial_x_train.shape)\n",
    "print(partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "passing-encyclopedia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "30/30 [==============================] - 3s 51ms/step - loss: 0.6926 - accuracy: 0.5041 - val_loss: 0.6904 - val_accuracy: 0.5069\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.6884 - accuracy: 0.5604 - val_loss: 0.6833 - val_accuracy: 0.7404\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.6792 - accuracy: 0.7826 - val_loss: 0.6674 - val_accuracy: 0.7978\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.6565 - accuracy: 0.8230 - val_loss: 0.6330 - val_accuracy: 0.7989\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.6124 - accuracy: 0.8334 - val_loss: 0.5807 - val_accuracy: 0.8101\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.5494 - accuracy: 0.8458 - val_loss: 0.5183 - val_accuracy: 0.8200\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.4779 - accuracy: 0.8535 - val_loss: 0.4618 - val_accuracy: 0.8303\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.4118 - accuracy: 0.8729 - val_loss: 0.4193 - val_accuracy: 0.8368\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.3612 - accuracy: 0.8798 - val_loss: 0.3903 - val_accuracy: 0.8439\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.3231 - accuracy: 0.8896 - val_loss: 0.3708 - val_accuracy: 0.8474\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.2929 - accuracy: 0.9002 - val_loss: 0.3582 - val_accuracy: 0.8518\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.2676 - accuracy: 0.9070 - val_loss: 0.3505 - val_accuracy: 0.8526\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.2412 - accuracy: 0.9186 - val_loss: 0.3451 - val_accuracy: 0.8530\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.2264 - accuracy: 0.9231 - val_loss: 0.3414 - val_accuracy: 0.8521\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.2066 - accuracy: 0.9292 - val_loss: 0.3397 - val_accuracy: 0.8530\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.1949 - accuracy: 0.9357 - val_loss: 0.3396 - val_accuracy: 0.8542\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.1847 - accuracy: 0.9403 - val_loss: 0.3404 - val_accuracy: 0.8535\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.1670 - accuracy: 0.9456 - val_loss: 0.3417 - val_accuracy: 0.8518\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.1597 - accuracy: 0.9483 - val_loss: 0.3441 - val_accuracy: 0.8522\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.1415 - accuracy: 0.9576 - val_loss: 0.3465 - val_accuracy: 0.8510\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습 시작\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eight-hydrogen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None) <dtype: 'float32'>\n",
      "(None, 1) <dtype: 'float32'>\n",
      "embedding_4 (None, None) float32\n",
      "global_max_pooling1d_2 (None, None, 16) float32\n",
      "dense_6 (None, 16) float32\n",
      "dense_7 (None, 8) float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(i.shape, i.dtype) for i in model.inputs]\n",
    "[print(o.shape, o.dtype) for o in model.outputs]\n",
    "[print(l.name, l.input_shape, l.dtype) for l in model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "regular-vietnam",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 3s - loss: 0.3630 - accuracy: 0.8461\n",
      "[0.36302122473716736, 0.8460800051689148]\n"
     ]
    }
   ],
   "source": [
    "# 학습이 끝난 모델을 테스트셋으로 평가\n",
    "results = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "sunrise-liability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    }
   ],
   "source": [
    "# history 변수에 model.fit 과정 중 train/val loss, accuracy 등 저장\n",
    "history_dict = history.history\n",
    "print(history_dict.keys()) # epoch에 따른 그래프를 그려볼 수 있는 항목들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "thermal-senator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "basic-endorsement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-statistics",
   "metadata": {},
   "source": [
    "Training and validation loss, accuracy를 그려 보면, 몇 epoch까지의 트레이닝이 적절한지 최적점을 추정해 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-baker",
   "metadata": {},
   "source": [
    "### 4.3 Word2Vec의 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-connecticut",
   "metadata": {},
   "source": [
    "- 워드임베딩: 감성분석의 비용을 절감하면서 정확도를 향상시킬 수 있는 자연어처리 기법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "burning-serve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = model.layers[0]\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "print(weights.shape)    # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "toxic-assignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 학습한 Embedding 파라미터를 파일에 써서 저장합니다. \n",
    "word2vec_file_path = os.getenv('HOME')+'/aiffel/sentiment_classification/data/word2vec.txt'\n",
    "f = open(word2vec_file_path, 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-4, word_vector_dim))  # 몇개의 벡터를 얼마 사이즈로 기재할지 타이틀을 씁니다.\n",
    "\n",
    "# 단어 개수(에서 특수문자 4개는 제외하고)만큼의 워드 벡터를 파일에 기록합니다. \n",
    "vectors = model.get_weights()[0]\n",
    "for i in range(4,vocab_size):\n",
    "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "democratic-begin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03590291, -0.03991875, -0.02156213, -0.02783189, -0.039451  ,\n",
       "       -0.04158521, -0.02779803, -0.03943876, -0.02372722, -0.03866684,\n",
       "       -0.03283311, -0.02591353, -0.01387497, -0.02491677, -0.03144612,\n",
       "       -0.02474207], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위에 남긴 임베딩 파라미터를 읽어서 word vector로 활용\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "word_vectors = Word2VecKeyedVectors.load_word2vec_format(word2vec_file_path, binary=False)\n",
    "vector = word_vectors['computer']\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-thirty",
   "metadata": {},
   "source": [
    "워드 벡터가 의미벡터 공간상에 유의미하게 학습되었는지 확인하기위해 유사도를 확인해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "executed-relation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('arguably', 0.9932642579078674),\n",
       " ('cats', 0.9891701340675354),\n",
       " ('award', 0.9884845614433289),\n",
       " ('contrast', 0.9874439239501953),\n",
       " ('favourite', 0.9862301349639893),\n",
       " ('simple', 0.9850082397460938),\n",
       " ('dirty', 0.9842315912246704),\n",
       " ('joins', 0.9822598695755005),\n",
       " ('noir', 0.9818346500396729),\n",
       " ('meadows', 0.9797354936599731)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-christopher",
   "metadata": {},
   "source": [
    "그리 좋은 결과는 아니다.. 우리가 다룬 정도의 훈련데이터로는 워드 벡터를 정교하게 학습시키기 어렵다.\n",
    "\n",
    "따라서 구글에서 제공하는 `Word2Vec`이라는 사전학습된 워드 임베딩 모델을 활용해본다.\n",
    "> __임베딩__<br>\n",
    "    임베딩(embedding)은 자연어를 숫자의 나열인 벡터로 바꾼 결과 혹은 그 일련의 과정 전체를 가리키는 용어입니다. 단어나 문장 각각을 벡터로 변환해 벡터 공간에 ‘끼워 넣는다(embed)’는 취지에서 임베딩이라는 이름이 붙었습니다.\n",
    "    임베딩에는 말뭉치(corpus)의 의미, 문법 정보가 응축돼 있습니다. 임베딩은 벡터이기 때문에 사칙연산이 가능하며, 단어/문서 관련도(relevance) 역시 계산할 수 있습니다.\n",
    "    \n",
    "> __전이학습__<br>\n",
    "    전이 학습이란 특정 문제를 풀기 위해 학습한 모델을 다른 문제를 푸는 데 재사용하는 기법을 의미합니다. 예컨대 대규모 말뭉치를 미리 학습(pretrain)한 임베딩을 문서 분류 모델의 입력값으로 쓰고, 해당 임베딩을 포함한 모델 전체를 문서 분류 과제를 잘할 수 있도록 업데이트(fine-tuning)하는 방식\n",
    "\n",
    "(https://ratsgo.github.io/natural%20language%20processing/2019/09/12/embedding/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-roman",
   "metadata": {},
   "source": [
    "Word2Vec 모델을 가져와 적용해보자.\n",
    "\n",
    "`$ ln -s ~/data/GoogleNews-vectors-negative300.bin.gz ~/aiffel/sentiment_classification/data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "swiss-controversy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.07421875e-01, -2.01171875e-01,  1.23046875e-01,  2.11914062e-01,\n",
       "       -9.13085938e-02,  2.16796875e-01, -1.31835938e-01,  8.30078125e-02,\n",
       "        2.02148438e-01,  4.78515625e-02,  3.66210938e-02, -2.45361328e-02,\n",
       "        2.39257812e-02, -1.60156250e-01, -2.61230469e-02,  9.71679688e-02,\n",
       "       -6.34765625e-02,  1.84570312e-01,  1.70898438e-01, -1.63085938e-01,\n",
       "       -1.09375000e-01,  1.49414062e-01, -4.65393066e-04,  9.61914062e-02,\n",
       "        1.68945312e-01,  2.60925293e-03,  8.93554688e-02,  6.49414062e-02,\n",
       "        3.56445312e-02, -6.93359375e-02, -1.46484375e-01, -1.21093750e-01,\n",
       "       -2.27539062e-01,  2.45361328e-02, -1.24511719e-01, -3.18359375e-01,\n",
       "       -2.20703125e-01,  1.30859375e-01,  3.66210938e-02, -3.63769531e-02,\n",
       "       -1.13281250e-01,  1.95312500e-01,  9.76562500e-02,  1.26953125e-01,\n",
       "        6.59179688e-02,  6.93359375e-02,  1.02539062e-02,  1.75781250e-01,\n",
       "       -1.68945312e-01,  1.21307373e-03, -2.98828125e-01, -1.15234375e-01,\n",
       "        5.66406250e-02, -1.77734375e-01, -2.08984375e-01,  1.76757812e-01,\n",
       "        2.38037109e-02, -2.57812500e-01, -4.46777344e-02,  1.88476562e-01,\n",
       "        5.51757812e-02,  5.02929688e-02, -1.06933594e-01,  1.89453125e-01,\n",
       "       -1.16210938e-01,  8.49609375e-02, -1.71875000e-01,  2.45117188e-01,\n",
       "       -1.73828125e-01, -8.30078125e-03,  4.56542969e-02, -1.61132812e-02,\n",
       "        1.86523438e-01, -6.05468750e-02, -4.17480469e-02,  1.82617188e-01,\n",
       "        2.20703125e-01, -1.22558594e-01, -2.55126953e-02, -3.08593750e-01,\n",
       "        9.13085938e-02,  1.60156250e-01,  1.70898438e-01,  1.19628906e-01,\n",
       "        7.08007812e-02, -2.64892578e-02, -3.08837891e-02,  4.06250000e-01,\n",
       "       -1.01562500e-01,  5.71289062e-02, -7.26318359e-03, -9.17968750e-02,\n",
       "       -1.50390625e-01, -2.55859375e-01,  2.16796875e-01, -3.63769531e-02,\n",
       "        2.24609375e-01,  8.00781250e-02,  1.56250000e-01,  5.27343750e-02,\n",
       "        1.50390625e-01, -1.14746094e-01, -8.64257812e-02,  1.19140625e-01,\n",
       "       -7.17773438e-02,  2.73437500e-01, -1.64062500e-01,  7.29370117e-03,\n",
       "        4.21875000e-01, -1.12792969e-01, -1.35742188e-01, -1.31835938e-01,\n",
       "       -1.37695312e-01, -7.66601562e-02,  6.25000000e-02,  4.98046875e-02,\n",
       "       -1.91406250e-01, -6.03027344e-02,  2.27539062e-01,  5.88378906e-02,\n",
       "       -3.24218750e-01,  5.41992188e-02, -1.35742188e-01,  8.17871094e-03,\n",
       "       -5.24902344e-02, -1.74713135e-03, -9.81445312e-02, -2.86865234e-02,\n",
       "        3.61328125e-02,  2.15820312e-01,  5.98144531e-02, -3.08593750e-01,\n",
       "       -2.27539062e-01,  2.61718750e-01,  9.86328125e-02, -5.07812500e-02,\n",
       "        1.78222656e-02,  1.31835938e-01, -5.35156250e-01, -1.81640625e-01,\n",
       "        1.38671875e-01, -3.10546875e-01, -9.71679688e-02,  1.31835938e-01,\n",
       "       -1.16210938e-01,  7.03125000e-02,  2.85156250e-01,  3.51562500e-02,\n",
       "       -1.01562500e-01, -3.75976562e-02,  1.41601562e-01,  1.42578125e-01,\n",
       "       -5.68847656e-02,  2.65625000e-01, -2.09960938e-01,  9.64355469e-03,\n",
       "       -6.68945312e-02, -4.83398438e-02, -6.10351562e-02,  2.45117188e-01,\n",
       "       -9.66796875e-02,  1.78222656e-02, -1.27929688e-01, -4.78515625e-02,\n",
       "       -7.26318359e-03,  1.79687500e-01,  2.78320312e-02, -2.10937500e-01,\n",
       "       -1.43554688e-01, -1.27929688e-01,  1.73339844e-02, -3.60107422e-03,\n",
       "       -2.04101562e-01,  3.63159180e-03, -1.19628906e-01, -6.15234375e-02,\n",
       "        5.93261719e-02, -3.23486328e-03, -1.70898438e-01, -3.14941406e-02,\n",
       "       -8.88671875e-02, -2.89062500e-01,  3.44238281e-02, -1.87500000e-01,\n",
       "        2.94921875e-01,  1.58203125e-01, -1.19628906e-01,  7.61718750e-02,\n",
       "        6.39648438e-02, -4.68750000e-02, -6.83593750e-02,  1.21459961e-02,\n",
       "       -1.44531250e-01,  4.54101562e-02,  3.68652344e-02,  3.88671875e-01,\n",
       "        1.45507812e-01, -2.55859375e-01, -4.46777344e-02, -1.33789062e-01,\n",
       "       -1.38671875e-01,  6.59179688e-02,  1.37695312e-01,  1.14746094e-01,\n",
       "        2.03125000e-01, -4.78515625e-02,  1.80664062e-02, -8.54492188e-02,\n",
       "       -2.48046875e-01, -3.39843750e-01, -2.83203125e-02,  1.05468750e-01,\n",
       "       -2.14843750e-01, -8.74023438e-02,  7.12890625e-02,  1.87500000e-01,\n",
       "       -1.12304688e-01,  2.73437500e-01, -3.26171875e-01, -1.77734375e-01,\n",
       "       -4.24804688e-02, -2.69531250e-01,  6.64062500e-02, -6.88476562e-02,\n",
       "       -1.99218750e-01, -7.03125000e-02, -2.43164062e-01, -3.66210938e-02,\n",
       "       -7.37304688e-02, -1.77734375e-01,  9.17968750e-02, -1.25000000e-01,\n",
       "       -1.65039062e-01, -3.57421875e-01, -2.85156250e-01, -1.66992188e-01,\n",
       "        1.97265625e-01, -1.53320312e-01,  2.31933594e-02,  2.06054688e-01,\n",
       "        1.80664062e-01, -2.74658203e-02, -1.92382812e-01, -9.61914062e-02,\n",
       "       -1.06811523e-02, -4.73632812e-02,  6.54296875e-02, -1.25732422e-02,\n",
       "        1.78222656e-02, -8.00781250e-02, -2.59765625e-01,  9.37500000e-02,\n",
       "       -7.81250000e-02,  4.68750000e-02, -2.22167969e-02,  1.86767578e-02,\n",
       "        3.11279297e-02,  1.04980469e-02, -1.69921875e-01,  2.58789062e-02,\n",
       "       -3.41796875e-02, -1.44042969e-02, -5.46875000e-02, -8.78906250e-02,\n",
       "        1.96838379e-03,  2.23632812e-01, -1.36718750e-01,  1.75781250e-01,\n",
       "       -1.63085938e-01,  1.87500000e-01,  3.44238281e-02, -5.63964844e-02,\n",
       "       -2.27689743e-05,  4.27246094e-02,  5.81054688e-02, -1.07910156e-01,\n",
       "       -3.88183594e-02, -2.69531250e-01,  3.34472656e-02,  9.81445312e-02,\n",
       "        5.63964844e-02,  2.23632812e-01, -5.49316406e-02,  1.46484375e-01,\n",
       "        5.93261719e-02, -2.19726562e-01,  6.39648438e-02,  1.66015625e-02,\n",
       "        4.56542969e-02,  3.26171875e-01, -3.80859375e-01,  1.70898438e-01,\n",
       "        5.66406250e-02, -1.04492188e-01,  1.38671875e-01, -1.57226562e-01,\n",
       "        3.23486328e-03, -4.80957031e-02, -2.48046875e-01, -6.20117188e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec_path = os.getenv('HOME')+'/aiffel/sentiment_classification/data/GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=1000000)\n",
    "vector = word2vec['computer']\n",
    "vector     # 무려 300dim의 워드 벡터입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-sleeping",
   "metadata": {},
   "source": [
    "메모리 에러 방지를 위해 `KeyedVectors.load_word2vec_format` 메소드로 워드 벡터를 로딩할 때 가장 많이 사용되는 상위 100만 개만 limt으로 조건을 주어 로딩하였다,\n",
    "메모리가 충분하다면 `limit=None`으로 모두 로딩한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "practical-statistics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loved', 0.6907792091369629),\n",
       " ('adore', 0.6816873550415039),\n",
       " ('loves', 0.6618633270263672),\n",
       " ('passion', 0.6100709438323975),\n",
       " ('hate', 0.600395679473877),\n",
       " ('loving', 0.5886635780334473),\n",
       " ('affection', 0.5664337873458862),\n",
       " ('undying_love', 0.5547305345535278),\n",
       " ('absolutely_adore', 0.5536839962005615),\n",
       " ('adores', 0.5440906882286072)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 메모리를 다소 많이 소비하는 작업이니 유의해 주세요.\n",
    "word2vec.similar_by_word(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-motel",
   "metadata": {},
   "source": [
    "이전 스텝에서 학습했던 모델의 임베딩 레이어를 Word2Vec의 것으로 교체하여 다시 학습시켜본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "retained-debate",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 300  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "\n",
    "# embedding_matrix에 Word2Vec 워드 벡터를 단어 하나씩마다 차례차례 카피한다.\n",
    "for i in range(4,vocab_size):\n",
    "    if index_to_word[i] in word2vec:\n",
    "        embedding_matrix[i] = word2vec[index_to_word[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "distinct-outdoors",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 580, 300)          3000000   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 574, 16)           33616     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 114, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 108, 16)           1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 3,035,569\n",
      "Trainable params: 3,035,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 300  # 워드 벡터의 차원 수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "# 모델 구성\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, \n",
    "                                 word_vector_dim, \n",
    "                                 embeddings_initializer=Constant(embedding_matrix),  # 카피한 임베딩을 여기서 활용\n",
    "                                 input_length=maxlen, \n",
    "                                 trainable=True))   # trainable을 True로 주면 Fine-tuning\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid')) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "operating-matter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "250/250 [==============================] - 18s 62ms/step - loss: 0.6805 - accuracy: 0.5368 - val_loss: 0.4164 - val_accuracy: 0.8155\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 0.3622 - accuracy: 0.8418 - val_loss: 0.3450 - val_accuracy: 0.8494\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 0.2197 - accuracy: 0.9203 - val_loss: 0.3256 - val_accuracy: 0.8682\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 0.1314 - accuracy: 0.9595 - val_loss: 0.3560 - val_accuracy: 0.8650\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 0.0722 - accuracy: 0.9817 - val_loss: 0.4105 - val_accuracy: 0.8622\n",
      "Epoch 6/20\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 0.0313 - accuracy: 0.9951 - val_loss: 0.4873 - val_accuracy: 0.8579\n",
      "Epoch 7/20\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 0.0150 - accuracy: 0.9987 - val_loss: 0.5227 - val_accuracy: 0.8620\n",
      "Epoch 8/20\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 0.0055 - accuracy: 0.9995 - val_loss: 0.5690 - val_accuracy: 0.8612\n",
      "Epoch 9/20\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 0.0020 - accuracy: 0.9999 - val_loss: 0.6026 - val_accuracy: 0.8615\n",
      "Epoch 10/20\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.6364 - val_accuracy: 0.8608\n",
      "Epoch 11/20\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 7.5507e-04 - accuracy: 1.0000 - val_loss: 0.6685 - val_accuracy: 0.8621\n",
      "Epoch 12/20\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 5.9934e-04 - accuracy: 1.0000 - val_loss: 0.7021 - val_accuracy: 0.8619\n",
      "Epoch 13/20\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 3.5328e-04 - accuracy: 1.0000 - val_loss: 0.7293 - val_accuracy: 0.8625\n",
      "Epoch 14/20\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 3.1151e-04 - accuracy: 1.0000 - val_loss: 0.7656 - val_accuracy: 0.8625\n",
      "Epoch 15/20\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 2.5662e-04 - accuracy: 1.0000 - val_loss: 0.7900 - val_accuracy: 0.8616\n",
      "Epoch 16/20\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 1.4683e-04 - accuracy: 1.0000 - val_loss: 0.8158 - val_accuracy: 0.8616\n",
      "Epoch 17/20\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 1.2973e-04 - accuracy: 1.0000 - val_loss: 0.8436 - val_accuracy: 0.8607\n",
      "Epoch 18/20\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 8.4592e-05 - accuracy: 1.0000 - val_loss: 0.8580 - val_accuracy: 0.8614\n",
      "Epoch 19/20\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 7.0815e-05 - accuracy: 1.0000 - val_loss: 0.8895 - val_accuracy: 0.8614\n",
      "Epoch 20/20\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 5.7076e-05 - accuracy: 1.0000 - val_loss: 0.9080 - val_accuracy: 0.8617\n"
     ]
    }
   ],
   "source": [
    "# 학습 진행\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20# 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=60,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "composite-monroe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 7s - loss: 0.9963 - accuracy: 0.8440\n",
      "[0.996309220790863, 0.8440399765968323]\n"
     ]
    }
   ],
   "source": [
    "# 테스트셋을 통한 모델 평가\n",
    "results = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
